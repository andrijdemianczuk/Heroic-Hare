{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b52ace02-f199-44bb-b5c3-2eaa35b6aa05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Custom Tools Demo <br/>\n",
    "<img src=\"https://eu-images.contentstack.com/v3/assets/blt6b0f74e5591baa03/blt5d5323f706ef288f/637c0195c162df49beaae102/Untitled_design_(77).png?width=1280&auto=webp&quality=95&format=jpg&disable=upscale\" width=500 /><br/>\n",
    "## Part 2: Assembling The Agentic Application\n",
    "Now that we've prototyped our agents in the previous notebook and worked through debugging them, we can start thinking about composing our application. This version is a little different than a GenieSpaces agentic workflow because we're relying on open libraries rather than the Databricks Genie Agents API. This time we'll need to take into consideration how we're invoking our agents and toolchains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ab39787-142c-4de1-b3f7-b05a698f24c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Designing Our Custom Application\n",
    "\n",
    "In this section, we'll be taking the tools we built previously and composing them into a fully agentic, compound AI system. Using the two prototype agents from the previous notebook, we'll be using the Databricks LangChain agent framework to create a fully-integrated system. Once it's working, we'll be registering our application in our MLFlow registry and serving a version of the model to a front-end application. To get an idea of the relationships between tools, toolboxes, node agents and supervisors, compare this project to a fully integrated GenieAgent version of a compound AI system: <br/>\n",
    "<br/>\n",
    "<img src=\"https://github.com/andrijdemianczuk/Heroic-Hare/blob/main/Notebooks/Custom%20Tools%20for%20Agents%20Demo/Compound%20AI%20Schematic.jpg?raw=true\" width=750/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f29be05b-d0a5-4c83-ab6b-7b9412c9b1ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Assembling our new agents into an compound AI system\n",
    "Since we've already prototyped our two agents (and by extension with two toolboxes) in the last notebook, we can start putting together our agentic application. We'll use the Databricks Agent framework to help us out with this. We'll need to coalesce and decouple our agents (including a supervisor) into new files and publish them to MLFlow for serving.\n",
    "\n",
    "### Building the artifact file\n",
    "Since we've already established the workflow of our tooling, we will need to create a coalesced application file. Technically we could break this up further into separate classes but for the sake of demonstration we will be exporting just a single file for agent registration as an artifact in Unity Catalog with MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b22f3c4d-9fd9-4784-9136-e74e297d1ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A quick note on the logic of this project\n",
    "Much of the same logic (and documentation) is directly repeated from the multi-agent composite demo. Where this differs is in its definition and use of custom tools and how they're being managed. Each tool is formatted for different queries that can be translated to other API calls, which was one of the most challenging aspects of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e8b0b19-7337-4b88-9d06-b28486e91fd2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Required Libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain-core langchain databricks-vectorsearch langchain-community feedparser\n",
    "%pip install -U -qqq langgraph==0.3.4 databricks-langchain databricks-agents uv\n",
    "%pip install mlflow\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7c41fb7-a25d-4396-a7eb-f044bd1cf351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## The asset file\n",
    "Most of the logic and processing is split up as naturally as possible in this notebook. In order to register our application as a 'model' in MLFlow it needs a definition of what that artifact looks like. Since we're not actually compiling a model (but rather an application), we need to build a pseudo file in place of something like a `pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818ad81c-a439-47f9-ae47-61a1e637d343",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Manage the Artifact File"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Check if the asset_agent.py file exists and delete it if it does\n",
    "#This is what allows us to run the notebook over again. If our logic changes, we can also ensure that we're not appending garbage to the file or causing it to fail.\n",
    "\n",
    "if os.path.exists(\"asset_agent.py\"):\n",
    "    os.remove(\"asset_agent.py\")\n",
    "    print(\"asset_agent.py has been deleted.\")\n",
    "else:\n",
    "    print(\"asset_agent.py does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb12c072-657e-49bb-8fac-ede78267b7ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dependencies and third-party libraries\n",
    "Assuming that this will be coalesced into a single file, it's also important that we manage our imports at the top of the file so all functions have access to what they need. I typically break my imports into blocks of related libraries to make management easier in the long-term. Adding category descriptors also helps keep things organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9e2e5c-54c8-4da1-822d-2236608242d5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Dependencies"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a asset_agent.py\n",
    "#Python libs\n",
    "import functools\n",
    "import os\n",
    "from typing import Any, Generator, Literal, Optional, Type\n",
    "import requests\n",
    "import feedparser\n",
    "\n",
    "#Databricks sdk & Databricks langchain implementation\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    ")\n",
    "# from databricks_langchain.genie import GenieAgent\n",
    "\n",
    "#Langchain tools (langraph is our agent lib)\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.agents import AgentType, initialize_agent, agent\n",
    "from langchain.tools import Tool\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "#MLflow stuff\n",
    "import mlflow\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "\n",
    "#Parsing libs\n",
    "from pydantic import BaseModel, Field\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79acf45b-206c-49b0-9ceb-e6693c64c5ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Defining our LLM Foundation Model\n",
    "The LLM Foundation Model provides the interpretation layer for internal communications between agents and the supervisor, as well as the user prompt and response. This is usually the point where we'd want to pick a good foundational model for our task. Remember that every AI application, no matter how general, still needs to have focus and intent for maintenance and stability. Databricks provides several registered models in the `databricks-uc` registry for use and is updating them globally on a regular basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec94050-6c0b-48c7-a26b-9cf0d30f87cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create an instance of our Foundation Model"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a asset_agent.py\n",
    "#This is the foundation LLM that we'll be using for the basis of our agents\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, extra_params={\"temperature\": 0.3})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1407694b-7610-4615-a347-7c6eea00641f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Defining our tools\n",
    "The following logic is the exact same logic we used in the prototypes of the previous notebook. The most important changes, however, are in the tool descriptions. They are slightly more verbose and descriptive here since we'll be relying on them for tool routing by the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fae2c0b0-6adb-4987-8dce-4913fe125da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Agent expectations (taken from our prototype explanation of expectations)\n",
    "Since we're going to be leveraging LangChain's BaseTool and BaseModel classes, we need to implement them properly. `BaseModel()` will provide us with an input interface and `BaseTool()` will provide us with the structured framework we will rely on for our application. It's important to choose an agent framework that's consistent with how we're going to make use of our agents later on. When we create an implementation of the `BaseTool()` class, we're obligated to include 2 attributes (`name` and `description`) and two behaviors (`_run` and `_arun`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a272fca-3e5f-49c9-b6e1-69849cb1b312",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Weather Tool Definition"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a asset_agent.py\n",
    "#Input Schema for the weather class. We'll be using this to compose the tool\n",
    "class WeatherInput(BaseModel):\n",
    "    latitude: float = Field(..., description=\"Latitude of the location\")\n",
    "    longitude: float = Field(..., description=\"Longitude of the location\")\n",
    "\n",
    "#Create an implementation inheriting the BaseTool abstract class from LangChain. _run() and _arun() are both required implementations. The name and description attributes are also required.\n",
    "class FetchWeatherTool(BaseTool):\n",
    "    name: str = \"fetch_weather\"\n",
    "    description: str = \"Fetch hourly weather temperature data for a given latitude and longitude. When asked about weather, assume that the user is always referring to temperature only. temperature_2m refers to the temperature in degrees celsius.\"\n",
    "    args_schema: Type[BaseModel] = WeatherInput #This is the input schema we defined above.\n",
    "\n",
    "    #The _run() function is always called when invoked. It's essentially doing the same thing as a class constructor, but since we're invoking the class from a toolchain in lieu of instancing the class as an object, this is run instead. This behaviour is what we're inheriting from the BaseTool() class.\n",
    "    def _run(self, latitude: float, longitude: float):\n",
    "        url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "        params = {\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"hourly\": \"temperature_2m\",\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"hourly\"][\"temperature_2m\"][:5]  # Preview\n",
    "        return f\"Failed to fetch data: {response.status_code}\"\n",
    "\n",
    "    def _arun(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Async not supported.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1907d5ed-5d07-452f-a60b-e519a8a09dfe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Document Search Tool Definition"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a asset_agent.py\n",
    "class ArxivSearchInput(BaseModel):\n",
    "    query: str = Field(..., description=\"Search keywords for the arXiv research papers\")\n",
    "    max_results: Optional[int] = Field(5, description=\"Max number of papers to return\")\n",
    "\n",
    "class SearchArxivTool(BaseTool):\n",
    "    name: str = \"search_arxiv\"\n",
    "    description: str = \"Search for academic papers on arXiv related to a given keyword.\"\n",
    "    args_schema: Type[BaseModel] = ArxivSearchInput\n",
    "\n",
    "    def _run(self, query: str, max_results: int = 3):\n",
    "        url = \"http://export.arxiv.org/api/query\"\n",
    "        params = {\n",
    "            \"search_query\": f\"all:{query}\",\n",
    "            \"start\": 0,\n",
    "            \"max_results\": max_results,\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        feed = feedparser.parse(response.text)\n",
    "        results = []\n",
    "        for entry in feed.entries[:max_results]:\n",
    "            results.append(f\"{entry.title} — {entry.link}\")\n",
    "        return \"\\n\".join(results) if results else \"No papers found.\"\n",
    "\n",
    "    def _arun(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Async not supported.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b324bd08-9e60-4081-8985-613a79a89c30",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Asset Search Tool Definition"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a asset_agent.py\n",
    "class AssetSearchInput(BaseModel):\n",
    "    asset_query: str = Field(..., description=\"The name of the location or asset to search for\")\n",
    "\n",
    "class SearchAssetsTool(BaseTool):\n",
    "    name: str = \"search_assets\"\n",
    "    description: str = \"Search for geographic or structural asset metadata using OpenStreetMap. \"\n",
    "    args_schema: Type[BaseModel] = AssetSearchInput\n",
    "\n",
    "    def _run(self, asset_query: str):\n",
    "        url = \"https://nominatim.openstreetmap.org/search\"\n",
    "        params = {\"q\": asset_query, \"format\": \"json\"}\n",
    "        headers = {\"User-Agent\": \"LangChainAgent/1.0 (andrij.demianczuk@databricks.com)\"}\n",
    "\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            return f\"Search API returned {response.status_code}. Cannot continue using search_assets.\"\n",
    "        \n",
    "        data = response.json()\n",
    "        if data:\n",
    "            result = data[0]\n",
    "            return f\"{result['display_name']} (lat: {result['lat']}, lon: {result['lon']})\"\n",
    "        else:\n",
    "            return f\"No results found for '{asset_query}'\"\n",
    "\n",
    "    def _arun(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Async not supported.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57678fbd-6f3c-46e9-8556-f32eecffe4e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Agent 1: Weather and Articles\n",
    "Our first agent will cover the duties of answering questions about weather with known coordinates and retrieving research articles. The purpose of bundling these two tools together is to show how we can have tools with different signatures being called upon in the same manner with predictable output (that is, they work). This capability is referred to as *idempotency* and is one of the critical concepts of object-oriented programming (polymorphism specifically). What we **want** is predictable behavior (even if the results are probabilistic) regardless of how the agent operates. To put it in another way, we need to make sure that our agents are engaged in a *declarative* manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9ed2d00-3f6d-4095-94d7-440b40667033",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create our weatherdoc agent"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a asset_agent.py\n",
    "#Specify what tools are available in the toolbox\n",
    "weather_tools = [\n",
    "    FetchWeatherTool(),\n",
    "    SearchArxivTool(),\n",
    "]\n",
    "\n",
    "#A good description so the supervisor knows what the node agent does and is responsible for.\n",
    "weatherdoc_agent_description = (\n",
    "    \"The Weather and Document agent specializes in retrieving weather (temperature) information from known coordinates. This agent can also retrieve documents from arxiv with a keyword search. This agent focuses on returning useful information to the prompter for weather and research articles. It can have internal conversations to get the most complete infomration from it's tools.\",\n",
    ")\n",
    "\n",
    "#Not directly used at this time, but handy to have for conversation tracking in the future.\n",
    "weather_doc_conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "#Create an instance of the agent.\n",
    "weatherdoc_bot = initialize_agent(\n",
    "    tools=weather_tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    max_iterations=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f44578e-cd7f-477a-b549-942eb4428fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Agent 2: Asset Lookup\n",
    "Notice here that the agent descriptions are much more verbose. Since we know they won't be interfaced with directly by the user, we need good descriptions to tell the parent (supervisor) agent what each one does. This is very similar in concept to a derived class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5707712b-5df5-405e-b6ba-48f3e355c046",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the Asset Search Agent"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a asset_agent.py\n",
    "asset_tools = [\n",
    "    SearchAssetsTool()\n",
    "]\n",
    "\n",
    "asset_agent_description = (\n",
    "    \"The asset agent looks up details for asset searches. Typically this will return coordinates but also contains information about the timezone for the assets as well. This input epxects a string for the query and is generally adept at major landmarks in North America.\",\n",
    ")\n",
    "\n",
    "asset_conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=10,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "asset_bot = initialize_agent(\n",
    "    tools=asset_tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    max_iterations=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a527568-9cad-4074-be69-cdb2c59c80bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Only used for debugging\n",
    "# asset_bot(\"Where is the Saddledome?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e1a4e58-21d5-49f7-b0ef-44eb819aed24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Only used for debugging\n",
    "# weatherdoc_bot(\"Can you find me some articles on NVidia's AI Technology?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "171b23d1-2c1b-42ff-955f-2150ea4b3b83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Defining the Supervisor Agent\n",
    "\n",
    "**THIS PART IS REALLY IMPORTANT**\n",
    "\n",
    "Now that we have all of our node agents defined for our application graph, let's tie them together with a supervisor agent. This agent is special because it has exclusive access to each of the agents. Although agents can technically talk directly to one another, having a brokerage is a pattern-safe design. We'll always try to decouple and encapsulate agents by responsibility for reasons of security, portability and generalizability. We want our agents to be aware enough of what they can do, but not so aware that they start creating their own graph edges. If we're considering object accountability from a design perspective this keeps the agents 'in line' with their least-privileged responsibility.\n",
    "\n",
    "In a nutshell, what we're doing is we're creating the supervisor agent as concrete implemenation of an iterator that builds it's runnable chain programmatically based on the presence of agents (`workers`) and the generated input description (hence `system_prompt` as the main directive of the application). The supervisor chain in the `supervisor_agent()` function essentially takes all of the upstream definitions and flattens them all into a single, deployable object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ec7757-7bfa-423a-8821-2a085869d4a0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define the Supervisor Agent"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a asset_agent.py\n",
    "#Update the max number of iterations between supervisor and worker nodes before returning to the user. This is how many internal 'conversations' the supervisor has with the other agents. This is a maximum value - if an answer is sufficient with fewer iterations, then great.\n",
    "MAX_ITERATIONS = 10\n",
    "\n",
    "#Add the description for each agent we're going to use as a dictionary\n",
    "worker_descriptions = {\n",
    "    \"Weatherdoc_Agent\":weatherdoc_agent_description,\n",
    "    \"Asset_Agent\":asset_agent_description,\n",
    "}\n",
    "\n",
    "#Flatten the descriptions into a single string variable\n",
    "formatted_descriptions = \"\\n\".join(\n",
    "    f\"- {name}: {desc}\" for name, desc in worker_descriptions.items()\n",
    ")\n",
    "\n",
    "#Tell the LM in plain language about the agents it has access to\n",
    "system_prompt = f\"Decide between routing between the following workers or ending the conversation if an answer is provided. \\n{formatted_descriptions}\"\n",
    "options = [\"FINISH\"] + list(worker_descriptions.keys())\n",
    "FINISH = {\"next_node\": \"FINISH\"}\n",
    "\n",
    "#Make use of all the above definitions and create the supervisor. This is what we'll be interfacing with and logging in MLFlow.\n",
    "def supervisor_agent(state):\n",
    "    count = state.get(\"iteration_count\", 0) + 1\n",
    "    if count > MAX_ITERATIONS:\n",
    "        return FINISH\n",
    "    \n",
    "    #Define our chaining logic\n",
    "    class nextNode(BaseModel):\n",
    "        next_node: Literal[tuple(options)]\n",
    "\n",
    "    #Assemble the entire chain, defining the supervisor and callable agents with some simple recursion logic.\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    )\n",
    "    supervisor_chain = preprocessor | llm.with_structured_output(nextNode)\n",
    "    next_node = supervisor_chain.invoke(state).next_node\n",
    "    \n",
    "    #If the response routed back to the same node, exit the loop. This identifies when the conversation has reached its peak epoch.\n",
    "    if state.get(\"next_node\") == next_node:\n",
    "        return FINISH\n",
    "    return {\n",
    "        \"iteration_count\": count,\n",
    "        \"next_node\": next_node\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1852b190-4941-4142-afb8-7695eddef1b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Defining the node graph\n",
    "This part seems more complicated than it really is. The first thing we need to do is define the conversational structure of the agent nodes and how they interact with the supervisor. Really, all we're doing here is invoking the agent node (any of them) and getting a response based on the input and classifying the response as an assistant response. Once we've done this for each of our agent nodes, we also do it for the supervisor agent and an assembled final answer. We wrap this all together under a `pyfunc()` banner so we can have one fully built, and bundled application. Then we just specify the entry and exit point and then run the whole object through a `compile()` function (part of the `LangGraph.graph` library) to package the whole thing up. Under the `pyfunc()` banner, we can also use MLFlow logging in the `databricks-uc` model registry.\n",
    "\n",
    "### Developer's note\n",
    "Most of the problems I encountered came up here. This was a tough one to debug because what was happening, was that the node agents weren't receiving any input from the supervisor. This was a pain to debug and it took a lot of time to cat out exactly what was going on in the stack traces at various points. My error stemmed from the fact that I was originally doing an implicit reference to the `llm_chain.prompt()` function, rather than building it up for each node agent programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "923423e4-2cb9-4463-aae5-2be213bc1046",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define and build the agent structure"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a asset_agent.py\n",
    "#This is the function that composes the message that interfaces with the LLM.\n",
    "def agent_node(state, agent, name, tools):\n",
    "    prompt = agent.agent.create_prompt(tools=tools)\n",
    "    agent.agent.llm_chain.prompt = prompt\n",
    "    user_messages = [msg for msg in state[\"messages\"] if msg[\"role\"] == \"user\"]\n",
    "    if not user_messages:\n",
    "        raise ValueError(\"No user message found to use as input\")\n",
    "\n",
    "    last_user_message = user_messages[-1][\"content\"]\n",
    "    result = agent.invoke({\"input\": last_user_message})\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"The {name} has determined: {result['output']}\",\n",
    "                \"name\": name,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "#This is the callable object that contains the response payload.\n",
    "def final_answer(state):\n",
    "    prompt = f\"\"\"You are the final summarizer.\n",
    "\n",
    "                Here is a conversation between a user and multiple assistant agents.\n",
    "\n",
    "                Each assistant agent was responsible for solving part of the user's question.\n",
    "\n",
    "                Your job is to answer the original question as clearly as possible based only on the assistant messages below.\n",
    "\n",
    "                Do not say \"I don't know\" unless no assistant provided a relevant answer.\n",
    "                If multiple assistants provided conflicting or redundant answers, pick the most confident and relevant one.\n",
    "\n",
    "                Respond to the user's original question with the best possible answer.\n",
    "                \"\"\"\n",
    "\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: state[\"messages\"] + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    final_answer_chain = preprocessor | llm\n",
    "    return {\"messages\": [final_answer_chain.invoke(state)]}\n",
    "\n",
    "\n",
    "#This object definition is technically just a struct to keep tabs on the agent.\n",
    "class AgentState(ChatAgentState):\n",
    "    next_node: str\n",
    "    iteration_count: int\n",
    "\n",
    "#Use a functools wrapper to build out the actual agent objects based on their descriptors\n",
    "weatherdoc_node = functools.partial(agent_node, agent=weatherdoc_bot, name=\"Weatherdoc_Agent\", tools=weather_tools)\n",
    "assetdoc_node = functools.partial(agent_node, agent=asset_bot, name=\"Asset_Agent\", tools=asset_tools)\n",
    "\n",
    "#Build the graph from the nodes, including something to send a result back to whatever's invoking the application (aka final answer).\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Weatherdoc_Agent\", weatherdoc_node)\n",
    "workflow.add_node(\"Asset_Agent\", assetdoc_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "workflow.add_node(\"final_answer\", final_answer)\n",
    "\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "# We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "for worker in worker_descriptions.keys():\n",
    "    workflow.add_edge(worker, \"supervisor\")\n",
    "\n",
    "# Let the supervisor decide which next node to go\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda x: x[\"next_node\"],\n",
    "    {**{k: k for k in worker_descriptions.keys()}, \"FINISH\": \"final_answer\"},\n",
    ")\n",
    "workflow.add_edge(\"final_answer\", END)\n",
    "multi_agent = workflow.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58cc6b37-0ae0-48b5-8bd2-ce00005638a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Adding a Chat interface\n",
    "Now that we have our asset_agent() object defined, all we need to do is create a parent function that colours the behaviour of our agent graph. This is defined as a class object that accepts the compiled agent graph as input once instanciated and decorates it with two behaviours. `predict()` is the public behaviour that interfaces with the user or external system. `predict_stream()` is used for the supervisor to keep track of, and maintain conversations with each of the agent nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0416260-da2f-43df-aa3b-d27b77c4920a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add a ChatAgent Wrapper"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a asset_agent.py\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    #Class constructor. This defines how the LangGraphChatAgent is initialized.\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    #This function is a behaviour that returns a response. It defines the chat structure between the agents. I.E., how they talk back and forth with the supervisor agent. We should probably create an installable library for this since it's pretty typical and can benefit from override and extension functionality.\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        request = {\n",
    "            \"messages\": [m.model_dump_compat(exclude_none=True) for m in messages]\n",
    "        }\n",
    "\n",
    "        messages = []\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        return ChatAgentResponse(messages=messages)\n",
    "\n",
    "    #This behaviour is how the supervisor keeps track of internal conversations. This is important as it allows agents to pass context to one another.\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        request = {\n",
    "            \"messages\": [m.model_dump_compat(exclude_none=True) for m in messages]\n",
    "        }\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                yield from (\n",
    "                    ChatAgentChunk(**{\"delta\": msg})\n",
    "                    for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9988bbf-654c-4806-8d25-91d0e9c1f6b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Logging to MLFlow\n",
    "Now that everything is assembled and put together, we can pass the whole thing into MLFlow. Since we've been appending everything to `asset_agent.py` up to this point, the entire definition for the application can actually be logged just like we would any other ML model. By adding auto-logging functionality to our application, we can utilize MLFlow for tracing our application and tracking decisions within conversations using inference tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a702fa-1348-4caa-bcf5-cf45ab9606cf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add the MLFlow Logger"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a asset_agent.py\n",
    "#Create the agent object, and specify it as the agent object to use when loading the agent back for inference via mlflow.models.set_model()\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = LangGraphChatAgent(multi_agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50817bd5-b41d-45b7-84d2-94a843153b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5778e7ef-6a4f-4743-bab2-4b0411efd56d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Interpreter"
    }
   },
   "outputs": [],
   "source": [
    "#Kill the python context to validate the file.\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a019a513-ae62-45ee-a8b9-f54af1e45b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a Personal Access Token (PAT) as a Databricks secret\n",
    "In order to access the underlying resources, we need to create a PAT\n",
    "- This can either be your own PAT or that of a System Principal ([AWS](https://docs.databricks.com/aws/en/dev-tools/auth/oauth-m2m) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/oauth-m2m)). You will have to rotate this token yourself upon expiry.\n",
    "- Add secrets-based environment variables to a model serving endpoint ([AWS](https://docs.databricks.com/aws/en/machine-learning/model-serving/store-env-variable-model-serving#add-secrets-based-environment-variables) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/store-env-variable-model-serving#add-secrets-based-environment-variables)).\n",
    "- You can reference the table in the deploy docs for the right permissions level for each resource: ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/deploy-agent#automatic-authentication-passthrough) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/deploy-agent#automatic-authentication-passthrough)).\n",
    "\n",
    "### Developer's Note\n",
    "For this application we shouldn't need to use the PAT that we previously used to access UC resources HOWEVER I'm likely going to create a GenieAgent soon to add to this project so I thought I'd leave it in for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79b2c023-8df3-4395-8443-b10185ec1325",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Managing Secrets Tokens"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dbruntime.databricks_repl_context import get_context\n",
    "\n",
    "#Set the variables for the PAT in the Databricks Secrets store\n",
    "secret_scope_name = \"general\"\n",
    "secret_key_name = \"genie_access\"\n",
    "\n",
    "#Inject the variables into the agent for use.\n",
    "os.environ[\"DB_MODEL_SERVING_HOST_URL\"] = \"https://\" + get_context().workspaceUrl\n",
    "assert os.environ[\"DB_MODEL_SERVING_HOST_URL\"] is not None\n",
    "os.environ[\"DATABRICKS_GENIE_PAT\"] = dbutils.secrets.get(\n",
    "    scope=secret_scope_name, key=secret_key_name\n",
    ")\n",
    "assert os.environ[\"DATABRICKS_GENIE_PAT\"] is not None, (\n",
    "    \"The DATABRICKS_GENIE_PAT was not properly set to the PAT secret\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b305f7b6-04c3-4570-bcf0-fd4fdbdee2df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Testing our compiled compount AI application\n",
    "Let's quickly run a test for each of our three tools, and both of our agents as defined. This is important and handy to understand how each agent expects to be invoked. This helps us understand the tooling of each agent and the stages of reasoning that occur.\n",
    "\n",
    "### Sample testing our application\n",
    "Now that we've written our entire application definition out to `multi_agent.py` and restarted our python interpreter, we can validate the source code that we prepped for deployment. All we need is a simple code stub to simulate how the serving endpoint interfaces with the application. This will show us a breakdown of how each agent is invoked based on different prompts. Experiment with the message prompts for different results. Once we're satisfied, we can consider deploying this to a production-grade serving endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7bd5dda-4be4-4ec6-bb82-781f75323e9d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test Weather Search"
    }
   },
   "outputs": [],
   "source": [
    "from asset_agent import AGENT\n",
    "\n",
    "input_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the weather at 51.0447° N, 114.0719° W?\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "AGENT.predict(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d35f0e-9703-4a50-9ff2-5a2425ffa49f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test Asset Search"
    }
   },
   "outputs": [],
   "source": [
    "from asset_agent import AGENT\n",
    "\n",
    "input_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the coordinates of the Calgary Saddledome?\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "AGENT.predict(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b992d27a-71d4-4052-8096-ba03eab31aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from asset_agent import AGENT\n",
    "\n",
    "input_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you find me the top 5 articles about Nvidia?\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "AGENT.predict(input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "215f1b98-cebd-4203-a901-8210ab3404c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log the agent as an MLflow model\n",
    "\n",
    "Log the agent as code from the `asset_agent.py` file. See [MLflow - Models from Code](https://mlflow.org/docs/latest/models.html#models-from-code).\n",
    "\n",
    "### Enable automatic authentication for Databricks resources\n",
    "For the most common Databricks resource types, Databricks supports and recommends declaring resource dependencies for the agent upfront during logging. This enables automatic authentication passthrough when you deploy the agent. With automatic authentication passthrough, Databricks automatically provisions, rotates, and manages short-lived credentials to securely access these resource dependencies from within the agent endpoint.\n",
    "\n",
    "To enable automatic authentication, specify the dependent Databricks resources when calling `mlflow.pyfunc.log_model().`\n",
    "  - **TODO**: If your Unity Catalog tool queries a [vector search index](docs link) or leverages [external functions](docs link), you need to include the dependent vector search index and UC connection objects, respectively, as resources. See docs ([AWS](https://docs.databricks.com/generative-ai/agent-framework/log-agent.html#specify-resources-for-automatic-authentication-passthrough) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/log-agent#resources))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b509b025-7ac9-4159-9a92-e1c98cbd3a25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating the MLFlow defintion\n",
    "When logging a model, project or application to MLFlow, we need to define a few components to tell MLFlow what the project 'looks' like. In other words, we need to provide the context around how it operates. This is what allows MLFlow to serve the project in a consistent and repeatable fashion. Basically, we define the tools and resources that are required for the project to run and be served. Since we're also relying on other Databricks resources (a serving endpoint and a few custom tools), those need to be both defined and present for the application to run. The same goes for the system and any AI tools that we defined in the code agent. Once all is in order, we can create our MLFlow run that logs our agent application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40037323-f02d-4508-9a0d-1c21779b8f9f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create and log an instance of the agent application in MLFlow"
    }
   },
   "outputs": [],
   "source": [
    "# Determine Databricks resources to specify for automatic auth passthrough at deployment time\n",
    "import mlflow\n",
    "from asset_agent import LLM_ENDPOINT_NAME, weather_tools, asset_tools\n",
    "from databricks_langchain import UnityCatalogTool, VectorSearchRetrieverTool\n",
    "from mlflow.models.resources import (\n",
    "    DatabricksFunction,\n",
    "    DatabricksGenieSpace,\n",
    "    DatabricksServingEndpoint,\n",
    ")\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "#This is what enables endpoint authentication. If this doesn't exist we won't be able to connect to any served endpoints providing this application.\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)\n",
    "]\n",
    "\n",
    "for tool in weather_tools:\n",
    "    if isinstance(tool, UnityCatalogTool):\n",
    "        resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
    "\n",
    "for tool in asset_tools:\n",
    "    if isinstance(tool, UnityCatalogTool):\n",
    "        resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
    "\n",
    "#Create our MLFlow run and log all teh things.\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"asset_agent\",\n",
    "        python_model=\"asset_agent.py\",\n",
    "        input_example=input_example,\n",
    "        extra_pip_requirements=[f\"databricks-connect=={get_distribution('databricks-connect').version}\"],\n",
    "        resources=resources\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af0acc4e-0aa6-4799-9380-55831b401612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register the model to Unity Catalog\n",
    "Once our model has been uploaded, we can then register it as the latest version and prepare it for serving. Here we define the location for the registered model in Unity Catalog, where it will be stored and managed as a knowledge object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d27dc1bf-78a2-4f1d-a9a5-8ec40639f7fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Register the model to UC"
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = \"ademianczuk\"\n",
    "schema = \"general\"\n",
    "model_name = \"asset_agent\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a817fbb-2c55-4280-8d2e-872b6b9fa92e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy our application\n",
    "The last step is to deploy our application to the Databricks serving endpoint. All we need to do is invoke the `deploy()` function from the databricks core library and pass in our actual access token that's stored in the secrets store. This delegates access to a privileged credential for the agent to act on the user's behalf. Creating the serving infrastructure can take a while (usually about 20 minute for first time deployment) because the entire isolated ephemeral network and container runtime are defined, built, deployed, configured and networked for the duration of the application serving lifetime. If an endpoint is terminated, the deployment descriptors remain from the previous configuration for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a551a9ed-5789-4d1a-865b-663ebe6efe42",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Deploy the App"
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "agents.deploy(\n",
    "    UC_MODEL_NAME,\n",
    "    uc_registered_model_info.version,\n",
    "    tags={\"endpointSource\": \"docs\"},\n",
    "    environment_vars={\n",
    "        \"DATABRICKS_GENIE_PAT\": f\"{{{{secrets/{secret_scope_name}/{secret_key_name}}}}}\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5dbe0c6-1a46-484e-913e-c0bb321aba99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "After your agent is deployed, you can chat with it in AI playground to perform additional checks, share it with SMEs in your organization for feedback, or embed it in a production application. See Databricks documentation ([AWS](https://docs.databricks.com/en/generative-ai/deploy-agent.html) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/deploy-agent))."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-Multi-Agent_Resoning_Demo_with_Custom_Tools",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
