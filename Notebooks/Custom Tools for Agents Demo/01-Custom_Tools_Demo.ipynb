{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e454c58e-2a65-4f2a-a4be-bbda229abd02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Custom Tools Demo\n",
    "<br/>\n",
    "<img src=\"https://eu-images.contentstack.com/v3/assets/blt6b0f74e5591baa03/blt5d5323f706ef288f/637c0195c162df49beaae102/Untitled_design_(77).png?width=1280&auto=webp&quality=95&format=jpg&disable=upscale\" width=500 /><br/>\n",
    "\n",
    "## Part 1: Building & Prototyping Agent Tools\n",
    "In this notebook we'll examine how we can build agents from simple API calls. Since we're not hard coding the API POST payload we need to few-shot prompt how we're going to do this with an example of the structure the API in question is expecting. This will allow us to build up a collection of tools. For the sake of demonstration we'll be separating out 3 tools across two different agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7015324-c078-43a6-bb07-91083be5ad7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Requirements install"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain-core langchain-databricks langchain databricks-vectorsearch langchain-community feedparser\n",
    "%pip install transformers sentence_transformers\n",
    "%pip install mlflow\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82d527fe-f097-4b9c-be36-7c723da88ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**LLMs (LLM + Prompt)**\n",
    "\n",
    "- A standalone LLM responds to text prompts based on knowledge from a vast training dataset.\n",
    "- Good for simple or generic queries but often disconnected from your real-world business data.\n",
    "<img src=\"https://learn.microsoft.com/en-us/azure/databricks/_static/images/generative-ai/llms.svg\" />\n",
    "\n",
    "**Hard-coded agent system (“Chain”)**\n",
    "\n",
    "- Developers orchestrate deterministic, pre-defined steps. For example, a RAG application can always retrieve from a vector store and combined results with the user prompt.\n",
    "- The logic is fixed, and the LLM does not decide which tool to call next.\n",
    "<img src=\"https://learn.microsoft.com/en-us/azure/databricks/_static/images/generative-ai/llm-tools.svg\" />\n",
    "\n",
    "**Tool-calling agent system**\n",
    "\n",
    "- The LLM decides which tool to use and when to use it at runtime.\n",
    "- This approach supports dynamic, context-aware decisions about which tools to invoke, such as a CRM database or a Slack posting API.\n",
    "<img src=\"https://learn.microsoft.com/en-us/azure/databricks/_static/images/generative-ai/ai-agent.svg\" />\n",
    "\n",
    "**Multi-agent systems**\n",
    "\n",
    "- Multiple specialized agents, each with its own function or domain.\n",
    "- A coordinator (sometimes an AI supervisor, sometimes rule-based) decides which agent to invoke at each step.\n",
    "- Agents can hand tasks off to each other while preserving the overall conversation flow.\n",
    "<img src=\"https://learn.microsoft.com/en-us/azure/databricks/_static/images/generative-ai/multi-agent-system.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77c2e875-24c3-44be-b210-c8d08292ead8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Simulating a few API calls\n",
    "We're going to build some tools around some API calls. For that we'll need to find a few good analogs for things like time series data\n",
    "<br/>\n",
    "<br/>\n",
    "**Simulate time series retrieval**<br/>\n",
    "\t•\tREST API for weather forecasts and historical data.<br/>\n",
    "\t•\tSimulates querying sensor data or time series like in Cognite.<br/>\n",
    "GET https://api.open-meteo.com/v1/forecast?latitude=51.0478&longitude=114.0593&hourly=temperature_2m<br/>\n",
    "<br/>\n",
    "**Simulate asset metadata catalog** <br/>\n",
    "This is a good example of proprietary asset storage. This could be coming either from an external API or a delta table. For the latter, I'd opt for using a Genie agent or an online table feature spec instead, but we can do it either way.<br/>\n",
    "\t•\tCan simulate fetching metadata about real-world locations (assets).<br/>\n",
    "\t•\tThink of this as querying a catalog of objects.<br/>\n",
    "GET https://nominatim.openstreetmap.org/search?q=Statue+of+Liberty&format=json<br/>\n",
    "<br/>\n",
    "**Simulate a document search**<br/>\n",
    "General search and lookup of online documentation. <br/>\n",
    "\t•\tUseful for document-based retrieval like CDF events or files.<br/>\n",
    "\t•\tGreat for agent retrieval demos with RAG.<br/>\n",
    "GET http://export.arxiv.org/api/query?search_query=all:ai&start=0&max_results=2<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd54a2ce-e440-469e-9c79-9907b45556a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Getting started with a prototype\n",
    "The fastest way to get started is to do a simple request call to each of the endpoints. If we put each of our APIs in their own function we can do rapid development with some declarative tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d65902b4-271a-4e67-9aa2-e3bdac4fcc58",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Testing some basic functions"
    }
   },
   "outputs": [],
   "source": [
    "#Let's create some sample functions to test our endpoints. We can call these just to get a sense of how they behave and what kind of input parameters we'll need to inject.\n",
    "\n",
    "import requests\n",
    "\n",
    "def fetch_weather(latitude: float, longitude: float):\n",
    "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"hourly\": \"temperature_2m\",\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "def search_assets(query: str):\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "    params = {\"q\": query, \"format\": \"json\"}\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "def search_arxiv(keywords: str, max_results: int = 3):\n",
    "    url = \"http://export.arxiv.org/api/query\"\n",
    "    params = {\n",
    "        \"search_query\": f\"all:{keywords}\",\n",
    "        \"start\": 0,\n",
    "        \"max_results\": max_results\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.text  # XML format, you can parse with `feedparser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8df482-eff9-47b5-8e57-f875a077353d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Quick preview of our data"
    }
   },
   "outputs": [],
   "source": [
    "#Let's do a quick preview of our data. The easiest way to do this is to throw it into a dataframe. We're just looking for a successful call and loose schema.\n",
    "# We can do the same for any of our API calls\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#Let's get a sample from Calgary\n",
    "pd.DataFrame(fetch_weather(51.0478, 114.0593))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea4a4f95-cebf-4c74-bc74-b9dab6568eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Structured data\n",
    "Obtaining structured data from an API makes the query results easier for the LLM to comprehend. Since there's little in terms of context required, LLMs tend to perform better and hallucinate less with structured or semi-structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e1c627f-c3f0-4c59-b3f7-416a8ccda66c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## (Option 1): Using LangChain's tools library to create tool stubs\n",
    "This is a pretty easy approach, using a declarative approach and LangChain's tool library. All you need to do is register a function with a name and detailed description that will help the agent understand what each tool does.\n",
    "\n",
    "This approach comes with a tradeoff; we lose flexibility with how our tools behave with the APIs. Since we'll want to tailor this behavior when we POST the payload, and retrieve only certain attributes this method is really only suitable for prototyping and rapid development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37dd77f-0fbf-462f-962e-652b03d8aee7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Easy tool registration with LangChain"
    }
   },
   "outputs": [],
   "source": [
    "#LangChain provides a quick and easy way to log functions as tools. This is an 'easy button' but we lose a degree of control in terms of behaviour management. Nonetheless, it's a pretty easy way to get going if you just want to do something simple.\n",
    "\n",
    "#Although we probably won't be using this set of tools, it's good to know that it's an option for rapid development or prototyping.\n",
    "\n",
    "from langchain.tools import Tool\n",
    "\n",
    "tool_box = [\n",
    "    Tool.from_function(fetch_weather, name=\"FetchWeather\", description=\"Fetch hourly weather data.\"),\n",
    "    Tool.from_function(search_assets, name=\"SearchAssets\", description=\"Search for asset-like locations.\"),\n",
    "    Tool.from_function(search_arxiv, name=\"SearchArxiv\", description=\"Search research documents.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d27dfddc-9bdf-43e1-a7e4-338a155aaf0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## (Option 2): Creating our own tools using LangChain's Abstract BaseTool() class\n",
    "There are some benefits to building our own tools based on abstract declarations from LangChain. Custom tools have added support for extended behaviours that simply aren't available when we register a tool using the from_function() .... function. This allows us to tailor our tools better to our needs, and we can build more complex functions that make external calls and have their own dependencies.\n",
    "\n",
    "### Tool chains and tool boxes\n",
    "**Toolboxes**\n",
    "- A toolbox is a collection of discrete tools or functions that an agent can call upon to complete tasks. We will be using these for our agents. \n",
    "- Toolboxes contain a collection of atomic functions that serve a common purpose. Generally speaking, an agent owns a single toolbox.\n",
    "\n",
    "- Tools are atomic: each tool does one thing well (e.g., run SQL, fetch from API, classify text).\n",
    "  - Tools are often defined with a schema: inputs/outputs are validated, often with decorators or Pydantic models.\n",
    "  - Tools may be:\n",
    "  - Internal (Python functions, DB queries)\n",
    "  - External (REST API calls, MLflow endpoints, vector search)\n",
    "  - Stateful (accessing memory, logs, or intermediate context)\n",
    "  - Agents choose tools based on prompts, planning modules, or learned decision policies.\n",
    "\n",
    "**Tool Chains**\n",
    "- A tool chain is an orchestrated sequence (or graph) of tools, where the output of one tool becomes the input for the next.\n",
    "- Tool chains are often implemented as a workflow or graph in compound AI systems.\n",
    "  - Sequential or branching logic (e.g., if → then → else).\n",
    "  - Deterministic or probabilistic planning (static routing vs planner-based).\n",
    "  - Can incorporate LLM-based planners or rule-based logic.\n",
    "  - Often built with LangGraph, LangChain chains, DSPy programs, or even custom DAGs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e1b6d8-9bfd-432d-a656-ef499c0f35ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Agent expectations\n",
    "Since we're going to be leveraging LangChain's BaseTool and BaseModel classes, we need to implement them properly. `BaseModel()` will provide us with an input interface and `BaseTool()` will provide us with the structured framework we will rely on for our application. It's important to choose an agent framework that's consistent with how we're going to make use of our agents later on. When we create an implementation of the `BaseTool()` class, we're obligated to include 2 attributes (`name` and `description`) and two behaviors (`_run` and `_arun`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d5509b0-1589-4c11-bbb5-cd018766686e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "A basic clean and typed tool definition - Weather Tool"
    }
   },
   "outputs": [],
   "source": [
    "#Note: normally we wouldn't worry about re-declaring our library imports, but in this case we are so the cells can be copied and pasted elsewhere to other notebooks.\n",
    "\n",
    "from langchain.tools import BaseTool\n",
    "from typing import Optional, Type\n",
    "from pydantic import BaseModel, Field\n",
    "import requests\n",
    "\n",
    "#Input Schema for the weather class. We'll be using this to compose the tool\n",
    "class WeatherInput(BaseModel):\n",
    "    latitude: float = Field(..., description=\"Latitude of the location\")\n",
    "    longitude: float = Field(..., description=\"Longitude of the location\")\n",
    "\n",
    "#Create an implementation inheriting the BaseTool abstract class from LangChain. _run() and _arun() are both required implementations. The name and description attributes are also required.\n",
    "class FetchWeatherTool(BaseTool):\n",
    "    name: str = \"fetch_weather\"\n",
    "    description: str = \"Fetch hourly weather temperature data for a given latitude and longitude. When asked about weather, assume that the user is always referring to temperature only. temperature_2m refers to the temperature in degrees celsius.\"\n",
    "    args_schema: Type[BaseModel] = WeatherInput #This is the input schema we defined above.\n",
    "\n",
    "    #The _run() function is always called when invoked. It's essentially doing the same thing as a class constructor, but since we're invoking the class from a toolchain in lieu of instancing the class as an object, this is run instead. This behaviour is what we're inheriting from the BaseTool() class.\n",
    "    def _run(self, latitude: float, longitude: float):\n",
    "        url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "        params = {\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"hourly\": \"temperature_2m\",\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"hourly\"][\"temperature_2m\"][:5]  # Preview\n",
    "        return f\"Failed to fetch data: {response.status_code}\"\n",
    "\n",
    "    def _arun(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Async not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81a6f23d-ab2b-4ece-8085-9bfecd1a79e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Repeating the process for the remaining tool defintions\n",
    "Next, we'll create a few more classes for the remaining tools. The structure is pretty much the same as the last agent we defined. We'll do one for the asset search and another for a document search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f475cda1-01d2-4822-aa76-267ff51ed070",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "A basic clean and typed tool definition - Document Search Tool"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from typing import Optional, Type\n",
    "from pydantic import BaseModel, Field\n",
    "import requests\n",
    "import feedparser\n",
    "\n",
    "\n",
    "class ArxivSearchInput(BaseModel):\n",
    "    query: str = Field(..., description=\"Search keywords for the arXiv research papers\")\n",
    "    max_results: Optional[int] = Field(5, description=\"Max number of papers to return\")\n",
    "\n",
    "class SearchArxivTool(BaseTool):\n",
    "    name: str = \"search_arxiv\"\n",
    "    description: str = \"Search for academic papers on arXiv related to a given keyword.\"\n",
    "    args_schema: Type[BaseModel] = ArxivSearchInput\n",
    "\n",
    "    def _run(self, query: str, max_results: int = 3):\n",
    "        url = \"http://export.arxiv.org/api/query\"\n",
    "        params = {\n",
    "            \"search_query\": f\"all:{query}\",\n",
    "            \"start\": 0,\n",
    "            \"max_results\": max_results,\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        feed = feedparser.parse(response.text)\n",
    "        results = []\n",
    "        for entry in feed.entries[:max_results]:\n",
    "            results.append(f\"{entry.title} — {entry.link}\")\n",
    "        return \"\\n\".join(results) if results else \"No papers found.\"\n",
    "\n",
    "    def _arun(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Async not supported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bcaec49-8031-43c7-9607-5bd2b8c84fc2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "A basic clean and typed tool definition - Search Assets Tool"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from typing import Optional, Type\n",
    "from pydantic import BaseModel, Field\n",
    "import requests\n",
    "\n",
    "class AssetSearchInput(BaseModel):\n",
    "    asset_query: str = Field(..., description=\"The name of the location or asset to search for\")\n",
    "\n",
    "class SearchAssetsTool(BaseTool):\n",
    "    name: str = \"search_assets\"\n",
    "    description: str = \"Search for geographic or structural asset metadata using OpenStreetMap. \"\n",
    "    args_schema: Type[BaseModel] = AssetSearchInput\n",
    "\n",
    "    def _run(self, asset_query: str):\n",
    "        url = \"https://nominatim.openstreetmap.org/search\"\n",
    "        params = {\"q\": asset_query, \"format\": \"json\"}\n",
    "        headers = {\"User-Agent\": \"LangChainAgent/1.0 (andrij.demianczuk@databricks.com)\"}\n",
    "\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            return f\"Search API returned {response.status_code}. Cannot continue using search_assets.\"\n",
    "        \n",
    "        data = response.json()\n",
    "        if data:\n",
    "            result = data[0]\n",
    "            return f\"{result['display_name']} (lat: {result['lat']}, lon: {result['lon']})\"\n",
    "        else:\n",
    "            return f\"No results found for '{asset_query}'\"\n",
    "\n",
    "    def _arun(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Async not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbed4243-1f36-4536-a0d3-b2f0abeda6de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Building an agent that will use the tools.\n",
    "Now that we have defined our tools, we must utilize them. We're going to need a few things to tie it all together including an instance of our LLM, an initializer and an agent description.\n",
    "\n",
    "Let's get started and make use of one of the UC-provided foundation models. For it's ease of implementation, we'll use Anthropic's Claude 3.7 Sonnet for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cca281a-a086-4d98-bab5-00a4d42fe76d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create an instance of the Databricks-hosted LLM"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_databricks import ChatDatabricks\n",
    "\n",
    "#This is the foundation LLM that we'll be using for the basis of our agents\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, extra_params={\"temperature\": 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0558bf0b-bc5a-4f01-b364-71473f1d0321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Testing the instance of the LLM\n",
    "Let's just run a quick test to make sure our foundation model endpoint is available and providing results. This is going to serve as the `router` for our tools and for our agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a0af885-6cb7-4e00-8b7c-d75cbc1b4c87",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test our instanced LLM object"
    }
   },
   "outputs": [],
   "source": [
    "print(llm.invoke('What are Calgarys coordinates?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a194c90-900e-42cb-96e6-7f048050307d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Agent Simulation 1: Weather & Research Articles\n",
    "The first agent we'll build will contain two tools, stored in a dynamic array (list). This collection of tools is referred to as a 'toolbox'. This prescribes what the agent has to work with. In our case, since we defined these tools as API calls, the agent is responsible for translating the conversation into something the APIs can understand. When we initialize our agent, it's also important to declare an agent type. This tells LangChain how the agent is expected to behave and requires certain parameters to work properly. How we declared our tools also has an impact on the agent type we choose. Since we're using a structured, memory-enabled format, we'll opt for a structured chat with zero-shot ReAct (reason-action) type description.\n",
    "\n",
    "A full explanation and list of LangChain agent types [can be found here](https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_types.AgentType.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd11151-15eb-45f3-9b74-ad61cb29d834",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create our first multi-tool agent"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "#Define the tools (toolbox) available to our agent\n",
    "tools = [\n",
    "    FetchWeatherTool(),\n",
    "    SearchArxivTool(),\n",
    "]\n",
    "\n",
    "# Initialize conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=10,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "aibot = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    max_iterations=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d90a42d-63f8-42d6-8ce1-0eb9e03657d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prompt engineering with tools\n",
    "Since our input prompts for the agents require a specific format based on the tools (remember, each API has it's own signature when called), we'll need to create a prompt based on the tooling interfaces. Using `llm_chain` tells our system that it has access to any of the tools in the toolbox. If we wanted to prescribe how the chaining of tools works, we'd create a static chain instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb38070c-dc4a-40d8-b68d-05c4ec5dc393",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a stub for our prompt"
    }
   },
   "outputs": [],
   "source": [
    "new_prompt = aibot.agent.create_prompt(\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "aibot.agent.llm_chain.prompt = new_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f89dc06-2973-4a49-9724-2773f4bde7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Agent Simulation 2: Custom Asset Lookups\n",
    "The second agent definition is much the same as the first, just with a differen toolbox available to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094eada3-538a-40d8-9f89-1cfbfff6363c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the second agent"
    }
   },
   "outputs": [],
   "source": [
    "#The second agent will be responsible for doing asset lookups. If we had a known set of our assets that our foundational model isn't aware of (e.g., OT asset locations in the field), we could use it to get our coordinates. When we create our composition of agents we'll declare descriptions for each agent.\n",
    "\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "#We're going to use this to create a second agent for a multi-agent approach\n",
    "asset_tools = [\n",
    "    SearchAssetsTool()\n",
    "]\n",
    "\n",
    "# Initialize conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=10,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "asset_bot = initialize_agent(\n",
    "    tools=asset_tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    max_iterations=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "050fc234-581a-4609-8281-698e2f904283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "asset_prompt = asset_bot.agent.create_prompt(\n",
    "    tools=asset_tools\n",
    ")\n",
    "\n",
    "asset_bot.agent.llm_chain.prompt = asset_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "437ea9ed-3ea0-4a75-95ad-6fa1726c993f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Testing our tools with both agents\n",
    "Now that we've created our two agent prototypes, we can run them through a litany of tests.\n",
    "\n",
    "### Warts and all.....\n",
    "Now, at the time of this writing there are still some bugs and limitations present in these agents and their logic. This is important to acknowledge as it shows development and growth both within the applicaiton and on a personal level. I believe it's essential to document these blemishes as they progress. Understanding systems is just as critical as being able to code them.\n",
    "\n",
    "- There is a bug when asking the asset agent for items not in its inventory. It loops until it times out without respecting the exit clause\n",
    "- The internal conversations are overly apologetic. This either has something to do with the llm temperature or the way I wrote the template. Will fix later.\n",
    "- Using defined agent types is going to be deprecated. [LangChain has recommended the updated use of LangGraph instead](https://python.langchain.com/docs/how_to/migrate_agent/). **This will be updated in the next release (as of 20/May/2025 - pinky promise!!!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aee8c3e-884b-4ba2-86d4-8e55b9a91f1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aibot_output = aibot(\"Can you find me some article references for AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6417b5cb-e25d-4f1b-a0ab-b4fb34c24574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(aibot_output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d76a50b-0648-4551-8a68-ee608419d005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aibot_output = aibot(\"Can you find me the address for the White House in Washington DC?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29b1b8ba-61c1-4d6c-95f8-5f4c69ac841a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(aibot_output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "522a3e04-af52-43d2-a289-1aed29a4e7b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aibot_output = aibot(\"What is the temperature at 51.0447° N, 114.0719° W?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e6c3a1-330b-42c7-9544-291772068bf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(aibot_output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1651973c-318b-4b8c-80c8-a4efc9b13065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aibot_output = aibot(\"What are the lat and lon coordinates of West Edmonton Mall?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef94381e-ecd8-4d3a-8963-4a95f7367cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(aibot_output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c956f625-58e5-46a5-82cd-8185cda4313c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aibot_output = aibot(\"What is the weather around West Edmonton Mall?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006e43d6-bb16-4c6e-b836-9751269978bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(aibot_output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a35c5206-210f-4b65-bf7c-1047106d3f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "asset_output = asset_bot(\"What are the coordinates of the Saddledome in Calgary AB?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b35a43f7-32d3-4dfc-a8cd-0700b3a2dcdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Assembling our new agents into an compound AI system\n",
    "Now that we have our two agents (and by extension with two toolboxes), we can start putting together our agentic application. We'll use the Databricks Agent framework to help us out with this. We'll need to coalesce and decouple our agents (including a supervisor) into new files and publish them to MLFlow for serving. Since this is a big task, we'll take care of that in the next notebook `02-Multi-Agent_Resoning_Demo_with_Custom_Tools`.\n",
    "\n",
    "### To be continued...\n",
    "In the next notebook `02-Multi-Agent_Resoning_Demo_with_Custom_Tools` we will take the logic and prototypes we built here and start composing a larger application that has access to both of our agents and by extension all of our tools for single publication. This is where we'll really getting into a true *compound* AI system - Where AI agents can talk to one another directly."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-Custom_Tools_Demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
