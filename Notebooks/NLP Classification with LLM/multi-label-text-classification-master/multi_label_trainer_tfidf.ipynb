{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33e4bb5a-ea67-43ac-84f8-95430f2730b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Q9ZG7WNCHNqw"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we will build a multi-label text classifier to predict the subject areas of arXiv papers from their abstract bodies. This type of classifier can be useful for conference submission portals like [OpenReview](https://openreview.net/). Given a paper abstract, the portal could provide suggestions on which areas the underlying paper would best belong to.\n",
    "\n",
    "The dataset was collected using the [`arXiv` Python library](https://github.com/lukasschwab/arxiv.py) that provides a wrapper around the [original arXiv API](http://arxiv.org/help/api/index). To know more, please refer to [this notebook](https://github.com/soumik12345/multi-label-text-classification/blob/master/arxiv_scrape.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39c5f952-823a-447e-8571-78b2395d2192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "5GICQpY-zws7"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae69449e-8098-4761-b585-092f8a536611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ho5uPff1fLoH"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c078c53-2d31-4742-91e2-c24c4cce6547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "qyeQtKSezymP"
   },
   "source": [
    "## Read data and perform basic EDA\n",
    "\n",
    "In this section, we first load the dataset into a `pandas` dataframe and then perform some basic exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1da92df3-3be7-4749-8870-4f73994e9b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "yFo2pNYbf2Du",
    "outputId": "a0478ffb-4612-4f5b-846b-8b33a502bfed"
   },
   "outputs": [],
   "source": [
    "arxiv_data = pd.read_csv(\n",
    "    \"https://github.com/soumik12345/multi-label-text-classification/releases/download/v0.2/arxiv_data.csv\"\n",
    ")\n",
    "arxiv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab5dcf74-0b15-4576-acdb-e0ae55b39234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "djk-kXWvHNq3"
   },
   "source": [
    "Our text features are present in the `summaries` column and their corresponding labels are in `terms`. As we can notice there are multiple categories associated with a particular entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "291c1fc3-87fb-426e-9654-c2de61a55ae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Em_8mJvUgKY-",
    "outputId": "8ce024c1-2745-4ab2-dd18-2b153784b4fc"
   },
   "outputs": [],
   "source": [
    "print(f\"There are {len(arxiv_data)} rows in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a914d211-cc02-4c71-86ba-af2bb19a6945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "P3bYjP_0HNq4"
   },
   "source": [
    "Real-world data is noisy. One of the most commonly observed such noise is data duplication. Here we notice that our initial dataset has got about 13k duplicate entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44a65a0a-ba4d-4ea3-944a-9355b8d3c147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9Vb9jtK5zjg",
    "outputId": "92e336c0-921a-4412-87ba-e3872333e3ff"
   },
   "outputs": [],
   "source": [
    "total_duplicate_titles = sum(arxiv_data[\"titles\"].duplicated())\n",
    "print(f\"There are {total_duplicate_titles} duplicate titles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbb4f826-13ea-4266-a53d-7b180b1bc639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "sEBbarGSHNq5"
   },
   "source": [
    "Before proceeding further we first drop these entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "647c6712-d1bb-4b6b-abb3-f31fbad98b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2259X-rf6OLY",
    "outputId": "08d73f4b-f0a0-46bd-9744-01b0e22f6a92"
   },
   "outputs": [],
   "source": [
    "arxiv_data = arxiv_data[~arxiv_data[\"titles\"].duplicated()]\n",
    "print(f\"There are {len(arxiv_data)} rows in the deduplicated dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "159742e8-bb39-4411-a079-403b7a8384ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TmgkCCr2g0w5",
    "outputId": "18a6bd58-e511-405f-c19e-15c40c8955c9"
   },
   "outputs": [],
   "source": [
    "# There are some terms with occurrence as low as 1.\n",
    "print(sum(arxiv_data[\"terms\"].value_counts() == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48579566-52da-40d5-93dd-ffff84bb5baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hyBJNHMdifJ-",
    "outputId": "0a098fa6-2723-42b9-b70b-f18bc9c11f3b"
   },
   "outputs": [],
   "source": [
    "# How many unique terms?\n",
    "print(arxiv_data[\"terms\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f98d330-547c-4ed9-9321-01bc3be4e1b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dPvNGph0HNq7"
   },
   "source": [
    "As observed above, out of 3157 unique combinations of `terms`, 2321 entries have the lowest occurrence. To prepare our train, validation, and test sets with [stratification](https://en.wikipedia.org/wiki/Stratified_sampling), we need to drop these terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34b9d7d8-5ffb-49eb-8e0d-f9a5b812eed6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77ZoCzrMhLxc",
    "outputId": "ee113f1e-23e3-4fd8-8ecd-4231df22da22"
   },
   "outputs": [],
   "source": [
    "# Filtering the rare terms.\n",
    "arxiv_data_filtered = arxiv_data.groupby(\"terms\").filter(lambda x: len(x) > 1)\n",
    "arxiv_data_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4125160b-a909-421b-b99b-962862672589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "MxrG9tim0QNr"
   },
   "source": [
    "## Convert the string labels to list of strings\n",
    "\n",
    "The initial labels are represented as raw strings. Here we make them `List[str]` for a more compact representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd21c2af-8e02-4f23-8096-1d4f10cc1d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIEGLc61iwbQ",
    "outputId": "e5612b8b-145c-45b7-f6ed-1273ec8bd538"
   },
   "outputs": [],
   "source": [
    "arxiv_data_filtered[\"terms\"] = arxiv_data_filtered[\"terms\"].apply(\n",
    "    lambda x: literal_eval(x)\n",
    ")\n",
    "arxiv_data_filtered[\"terms\"].values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bd1c8f1-ef9b-4632-80aa-5a864da0cc41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zjFB8Uoo0cXM"
   },
   "source": [
    "## Stratified splits because of class imbalance\n",
    "\n",
    "The dataset has a [class imbalance problem](https://developers.google.com/machine-learning/glossary/#class-imbalanced-dataset). So, to have a fair evaluation result, we need to ensure the datasets are sampled with stratification. To know more about different strategies to deal with the class imbalance problem, you can follow [this tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data). For an end-to-end demonstration of classification with imbablanced data, refer to [Imbalanced classification: credit card fraud detection](https://keras.io/examples/structured_data/imbalanced_classification/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49e5bfbf-816e-46ca-8f9d-f606afcc71fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EbKDVTKPgOKe",
    "outputId": "f16da845-9641-42dd-c387-649facc014a9"
   },
   "outputs": [],
   "source": [
    "test_split = 0.1\n",
    "\n",
    "# Initial train and test split.\n",
    "train_df, test_df = train_test_split(\n",
    "    arxiv_data_filtered,\n",
    "    test_size=test_split,\n",
    "    stratify=arxiv_data_filtered[\"terms\"].values,\n",
    ")\n",
    "\n",
    "# Splitting the test set further into validation\n",
    "# and new test sets.\n",
    "val_df = test_df.sample(frac=0.5)\n",
    "test_df.drop(val_df.index, inplace=True)\n",
    "\n",
    "print(f\"Number of rows in training set: {len(train_df)}\")\n",
    "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
    "print(f\"Number of rows in test set: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e10d5efc-1ab6-43be-b852-4551fa903362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "96Ew2PPI0lVc"
   },
   "source": [
    "## Multi-label binarization\n",
    "\n",
    "Now we preprocess our labels using [`MultiLabelBinarizer`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4322324-8c17-4883-b0f3-58f5b49dd512",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vgxbdwGf07E",
    "outputId": "1786e58c-9f56-4c14-f9cb-1f4da7fdad85"
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit_transform(train_df[\"terms\"])\n",
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89ada701-4c84-40af-a412-948132b82617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "HTcmRnP9HNq9"
   },
   "source": [
    "`MultiLabelBinarizer`separates out the individual unique classes available from the label pool and then uses this information to represent a given label set with 0's and 1's. Below is an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da791ffd-4b41-4735-b3ad-7fb6c2916abe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfMO31s8HNq9",
    "outputId": "7a6b1cb9-761b-409c-fbc5-9534f5168629"
   },
   "outputs": [],
   "source": [
    "sample_label = train_df[\"terms\"].iloc[0]\n",
    "print(f\"Original label: {sample_label}\")\n",
    "\n",
    "label_binarized = mlb.transform([sample_label])\n",
    "print(f\"Label-binarized representation: {label_binarized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c890393-833f-46d5-8ef7-cdf53307e3b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "a2kFVBCG0oXV"
   },
   "source": [
    "## Data preprocessing and `tf.data.Dataset` objects\n",
    "\n",
    "We first get percentile estimates of the sequence lengths. The purpose will be clear in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d35e9ae9-ce18-425c-bb6a-cd978bf255f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCR-_Iw3gyT-",
    "outputId": "1e00b7b1-b70a-4f2f-f97a-4defa37d3646"
   },
   "outputs": [],
   "source": [
    "train_df[\"summaries\"].apply(lambda x: len(x.split(\" \"))).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bc77be1-beb7-4660-bfc3-df82feaf6665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "2rdivJop02xG"
   },
   "source": [
    "Notice that 50% of the abstracts have a length of 154 (you may get a different number based on the split). So, any number near that is a good enough approximate for the maximum sequence length. \n",
    "\n",
    "Now, we write utilities to prepare our datasets that would go straight to the text classifier model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "188eacef-8b3c-4134-b369-db8435058bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "QoMgEZrtVBDS"
   },
   "outputs": [],
   "source": [
    "max_seqlen = 150\n",
    "batch_size = 128\n",
    "padding_token = \"<pad>\"\n",
    "\n",
    "\n",
    "def unify_text_length(text, label):\n",
    "    # Split the given abstract and calculate its length.\n",
    "    word_splits = tf.strings.split(text, sep=\" \")\n",
    "    sequence_length = tf.shape(word_splits)[0]\n",
    "    \n",
    "    # Calculate the padding amount.\n",
    "    padding_amount = max_seqlen - sequence_length\n",
    "    \n",
    "    # Check if we need to pad or truncate.\n",
    "    if padding_amount > 0:\n",
    "        unified_text = tf.pad([text], [[0, padding_amount]], constant_values=\"<pad>\")\n",
    "        unified_text = tf.strings.reduce_join(unified_text, separator=\"\")\n",
    "    else:\n",
    "        unified_text = tf.strings.reduce_join(word_splits[:max_seqlen], separator=\" \")\n",
    "    \n",
    "    # The expansion is needed for subsequent vectorization.\n",
    "    return tf.expand_dims(unified_text, -1), label\n",
    "\n",
    "\n",
    "def make_dataset(dataframe, is_train=True):\n",
    "    label_binarized = mlb.transform(dataframe[\"terms\"].values)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (dataframe[\"summaries\"].values, label_binarized)\n",
    "    )\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "    dataset = dataset.map(unify_text_length).cache()\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de8f71d6-2a59-44ae-8a43-55d0b78899aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fi5g9v3HHNq-"
   },
   "source": [
    "Now we can prepare the `tf.data.Dataset` objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43f6d662-43f2-48cc-9934-84b247b4e6bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "F_vrgkCXrWOS"
   },
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_df, is_train=True)\n",
    "validation_dataset = make_dataset(val_df, is_train=False)\n",
    "test_dataset = make_dataset(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fed15b43-6cb4-4d2b-8eaf-5deff8f9e8d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "-1Bb4Xnm1EwK"
   },
   "source": [
    "## Dataset preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e945d54-b631-479a-82a0-e30ba9b3c449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-8k7gScVoz6",
    "outputId": "8ea5d01c-e0ac-4e8b-b15b-b4b1975cd979"
   },
   "outputs": [],
   "source": [
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text[0]}\")\n",
    "    print(f\"Label(s): {mlb.inverse_transform(label)[0]}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bc59f0a-4b08-4824-ac27-da3d17844f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Yfbnoi-y1IoB"
   },
   "source": [
    "## Vocabulary size for vectorization\n",
    "\n",
    "Before we feed the data to our model we need to represent them as numbers. For that purpose, we will use the [`TextVectorization` layer](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization). It can operate as a part of your main model so that the model is excluded from the core preprocessing logic. This greatly reduces the chances of training and serving skew. \n",
    "\n",
    "We first calculate the number of unique words present in the abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4327e24e-e245-4f54-ab16-ffc4974d2c24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IIW42KOgyfE7",
    "outputId": "69e504dc-a55e-41ec-e2b9-11b35750d9ca"
   },
   "outputs": [],
   "source": [
    "train_df[\"total_words\"] = train_df[\"summaries\"].str.split().str.len()\n",
    "vocabulary_size = train_df[\"total_words\"].max()\n",
    "print(f\"Vocabulary size: {vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d79cee7e-3f74-496c-a62a-71c542224982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "nis2d-xFHNq_"
   },
   "source": [
    "Now we can create our text classifier model with the `TextVectorization` layer present inside it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ba30126-da29-4182-8a4b-16855a8abf36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3TBUFHm11M3G"
   },
   "source": [
    "## Create model with `TextVectorization`\n",
    "\n",
    "A batch of raw text will first go through the `TextVectorization` layer and it will generate their integer representations. Internally, the `TextVectorization` layer will first create bi-grams out of the sequences and then represent them using [TF-IDF](https://wikipedia.org/wiki/Tf%E2%80%93idf). The output representations will then be passed to the shallow model responsible for text classification. \n",
    "\n",
    "To know more about other possible configurations with `TextVectorizer`, please consult the [official documentation](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96821e45-de1b-4ce9-8be3-08d03054506f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "4XX7ovyPokNs"
   },
   "outputs": [],
   "source": [
    "text_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size, ngrams=2, output_mode=\"tf_idf\"\n",
    ")\n",
    "\n",
    "# `TextVectorization` needs to be adapted as per the vocabulary from our\n",
    "# training set.\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    text_vectorizer.adapt(train_dataset.map(lambda text, label: text))\n",
    "\n",
    "\n",
    "def make_model():\n",
    "    shallow_mlp_model = keras.Sequential(\n",
    "        [\n",
    "            text_vectorizer,\n",
    "            layers.Dense(512, activation=\"relu\"),\n",
    "            layers.Dense(256, activation=\"relu\"),\n",
    "            layers.Dense(len(mlb.classes_), activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    return shallow_mlp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54229208-5234-486b-8822-bd9df894b8bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "i7i71RLtugUs"
   },
   "source": [
    "Without the CPU placement, we run into: \n",
    "\n",
    "```\n",
    "(1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13039479-bf20-4c80-a59f-2032110b519e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58tUdCsQuI1P",
    "outputId": "47a81fa0-d83c-4dfe-8745-8122ebbc4da9"
   },
   "outputs": [],
   "source": [
    "shallow_mlp_model = make_model()\n",
    "shallow_mlp_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c27eeab-f420-4bbd-845b-42fc771c2cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "y1Hr9D0O1Tw0"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "We will train our model using the binary cross-entropy loss. This is because the labels are not disjoint. For a given abstract, we may have multiple categories. So, we will divide the prediction task into a series of multiple binary classification problems. This is also why we kept the activation function of the classification layer in our model to sigmoid. Researchers have used other combinations of loss function and activation function as well. For example, in [Exploring the Limits of Weakly Supervised Pretraining](https://arxiv.org/abs/1805.00932), Mahajan et al. used the softmax activation function and cross-entropy loss to train their models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25c78b30-6d01-40d4-ae0b-c5735eced55f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCoaRkA-wsC3",
    "outputId": "d8f31c73-6f8b-43db-e226-c6fd55e446df"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "shallow_mlp_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"categorical_accuracy\"]\n",
    ")\n",
    "\n",
    "history = shallow_mlp_model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac22fe30-c011-4d4e-b582-4f4ee88f8e01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "bOG4RF0KxUpq",
    "outputId": "4b809603-f59c-4c4b-9f89-2d03cc42ee4a"
   },
   "outputs": [],
   "source": [
    "def plot_result(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"categorical_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b575402-467d-470a-a58b-8dd402c4e299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "XkwRAIcsNeXj"
   },
   "source": [
    "While training, we notice an initial sharp fall in the loss followed by a gradual decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "475153ea-170a-413a-8a8b-f9698b4d7623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wBvqeuk88G9r"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2df07dda-1973-4c3b-9f4d-4c0d901c4b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxz8yDaT8MdL",
    "outputId": "04cbcc20-9c7d-417a-fc0f-f67236533711"
   },
   "outputs": [],
   "source": [
    "_, categorical_acc = shallow_mlp_model.evaluate(test_dataset)\n",
    "print(f\"Categorical accuracy on the test set: {round(categorical_acc * 100, 2)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "182a6c45-fbdc-44c8-8d77-081855c79d01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "of0EtUhKNjv6"
   },
   "source": [
    "The trained model gives us a validation accuracy of ~70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22201729-b48e-4173-830a-320381f0cfe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "SeCfK4daFGJq"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b37edcf3-71da-42da-b37c-266aa0256a0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axBPAMpe-sp2",
    "outputId": "3ba99084-0491-47ce-dce4-f74b69708e7e"
   },
   "outputs": [],
   "source": [
    "text_batch, label_batch = next(iter(test_dataset))\n",
    "predicted_probabilities = shallow_mlp_model.predict(text_batch)\n",
    "\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text[0]}\")\n",
    "    print(f\"Label(s): {mlb.inverse_transform(label)[0]}\")\n",
    "    predicted_proba = [proba for proba in predicted_probabilities[i]]\n",
    "    top_3_labels = [x for _, x in sorted(zip(predicted_probabilities[i], mlb.classes_), key=lambda pair: pair[0], reverse=True)][:3]\n",
    "    print(f\"Predicted Label(s): ({', '.join([label for label in top_3_labels])})\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95e875de-481a-4f93-8e2d-d0ca14660618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "5-gQfc11SwAv"
   },
   "source": [
    "The prediction results are not that great but not below the par for a simple model like ours. We can improve this performance with models that consider word order like LSTM or even those that use Transformers ([Vaswani et al.](https://arxiv.org/abs/1706.03762))."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "multi_label_trainer_tfidf",
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [],
   "name": "multi_label_trainer_tfidf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
